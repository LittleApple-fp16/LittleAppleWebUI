import io
import logging
import sys
import matplotlib
import os
import gradio as gr
import random
import json
import glob
import numpy
from get_pixiv_ref_token import get_ref_token
from typing import Literal, cast
from tqdm import tqdm
from tqdm.contrib import tzip
from waifuc.action import HeadCountAction, AlignMinSizeAction, CCIPAction, ThreeStageSplitAction, ModeConvertAction, ClassFilterAction, PersonSplitAction, TaggingAction, RatingFilterAction, NoMonochromeAction
from waifuc.export import SaveExporter
from waifuc.source import DanbooruSource, PixivSearchSource, ZerochanSource, LocalSource
from PIL import Image
from train import run_train_plora
from imgutils.data import load_image, load_images, rgb_encode, rgb_decode
from imgutils.tagging import get_wd14_tags, get_mldanbooru_tags, drop_blacklisted_tags, drop_overlap_tags, tags_to_text
from imgutils.metrics import ccip_difference, ccip_clustering, lpips_clustering
from imgutils.operate import censor_areas, squeeze, squeeze_with_transparency
from imgutils.detect import detect_faces, detect_heads, detection_visualize, detect_person
from imgutils.segment import segment_rgba_with_isnetis
from cyberharem.publish.convert import convert_to_webui_lora

matplotlib.use('Agg')


def download_images(source_type, character_name, p_min_size, p_background, p_class, p_rating, p_crop_person, p_auto_tagging, num_images, ptoken, p_ai):
    global output_cache
    character_name = character_name.replace(' ', '_')  # å°†ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    save_path = 'dataset/' + character_name
    print("\n - å¼€å§‹è·å–æ•°æ®é›†")
    if source_type == 'Danbooru':
        source_init = DanbooruSource([character_name, 'solo'])
    elif source_type == 'Pixiv':
        source_init = PixivSearchSource(
            character_name,
            no_ai=p_ai,
            refresh_token=ptoken
        )
        source_init.attach(CCIPAction(source_init))  # ç”¨äºè¿‡æ»¤pixivä¼ å›çš„ä¸ç›¸å…³è§’è‰²å›¾åƒ
    elif source_type == 'Zerochan':
        source_init = ZerochanSource([character_name, 'solo'])
    else:
        output_cache = []
        return "å›¾ç«™é”™è¯¯"
    rating_map = {0: 'safe', 1: 'r15', 2: 'r18'}
    class_map = {1: 'illustration', 2: 'bangumi'}
    ratings_to_filter = set(rating_map.values()) - set([rating_map[i] for i in p_rating if i in rating_map])

    if p_class:
        if 0 in p_class:
            source_init.attach(NoMonochromeAction())
        class_to_filter = set(class_map.values()) - set([class_map[i] for i in p_class if i in class_map])
        source_init.attach(ClassFilterAction(cast(list[Literal['illustration', 2: 'bangumi']], list(ratings_to_filter))), RatingFilterAction(cast(list[Literal['safe', 'r15', 'r18']], list(class_to_filter))))  # è¿‡æ»¤å…·æœ‰è¯„çº§çš„å›¾ç‰‡
    if p_crop_person:
        source_init.attach(PersonSplitAction())
    if p_auto_tagging:
        source_init.attach(TaggingAction(force=True))
    source_init.attach(
        AlignMinSizeAction(int(p_min_size)),
        ModeConvertAction('RGB', p_background),

        HeadCountAction(1),  # åªä¿ç•™ä¸€å¤´çš„å›¾ç‰‡
    )[:num_images].export(  # åªä¸‹è½½å‰num_imageså¼ å›¾ç‰‡
        SaveExporter(save_path)  # å°†å›¾ç‰‡ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    )
    print(ratings_to_filter)
    output_cache = []
    return "å·²å°†è·å–å›¾ç‰‡ä¿å­˜è‡³" + save_path


def dataset_getImg(dataset_name):  # è¯·ç¡®ä¿æ¯ä¸ªæ–¹æ³•ä¸­åªè°ƒç”¨ä¸€æ¬¡ ç”±äºtqdm
    global output_cache
    # print(" - å¼€å§‹åŠ è½½å›¾åƒ")
    dataset_path = "dataset/" + dataset_name
    images = []
    img_name = []
    for filename in tqdm(os.listdir(dataset_path), file=sys.stdout, desc=" - å¼€å§‹åŠ è½½å›¾åƒ"):
        if filename.endswith(('.png', '.jpg', '.jpeg')):
            img = Image.open(os.path.join(dataset_path, filename))
            if img is not None:
                images.append(img)
                img_name.append(filename)
    # output_cache = images
    images = load_images(images)
    return images, img_name


def has_image(got_list):
    if any(isinstance(item, Image.Image) for item in got_list):
        return True
    else:
        return False


def clustering(dataset_name):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    # print(clusters)
    clustered_imgs = []
    added_clusters = set()  # åˆ›å»ºä¸€ä¸ªé›†åˆ å…¶ä¸­å­˜å‚¨å·²ç»æ·»åŠ è¿‡çš„æ ‡ç­¾ æ­¤é›†åˆå°†çº¦æŸè¢«è¿‡æ»¤çš„imgåˆ—è¡¨ é›†åˆä¸­çš„å…ƒç´ æ— æ³•dup
    # print(" - å·®åˆ†è¿‡æ»¤å¼€å§‹å¤„ç†")
    for i, cluster in enumerate(tqdm(lpips_clustering(images), file=sys.stdout, desc=" - å·®åˆ†è¿‡æ»¤å¼€å§‹å¤„ç†", ascii="â–‘â–’â–ˆ")):  # èšç±»æ–¹æ³• -1è¡¨ç¤ºnoiseï¼Œä¸sklearnä¸­çš„ç›¸åŒ
        if cluster == -1:
            clustered_imgs.append(images[i])
        elif cluster not in added_clusters:
            clustered_imgs.append(images[i])
            added_clusters.add(cluster)
    output_cache = clustered_imgs
    return clustered_imgs


def three_stage(dataset_name):
    global output_cache
    if dataset_name.endswith("_processed"):
        process_dir = f"dataset/{dataset_name}"
    else:
        process_dir = f"dataset/{dataset_name}_processed"
    local_source = LocalSource(f"dataset/{dataset_name}")
    local_source.attach(
        ThreeStageSplitAction(),
    ).export(SaveExporter(process_dir, True))
    output_cache = []
    return "å·²ä¿å­˜è‡³"+process_dir+"æ–‡ä»¶å¤¹"

# def person_detect(dataset_name, level, version, max_infer_size, conf_threshold, iou_threshold):
#     global output_cache
#     images = dataset_getImg(dataset_name)[0]
#     detected = []
#     if level:
#         level = "m"
#     else:
#         level = "n"
#     print(" - äººç‰©æ£€æµ‹å¼€å§‹å¤„ç†")
#     for img in tqdm(images):
#         detected.append(detect_person(img, level, version, max_infer_size, conf_threshold, iou_threshold))
#     output_cache = detected
#     return detected


def face_detect(dataset_name, level, version, max_infer_size, conf_threshold, iou_threshold):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    detected = []
    if level:
        level = "s"
    else:
        level = "n"
    # print(" - é¢éƒ¨æ£€æµ‹å¼€å§‹å¤„ç†")
    # print("   *å°†è¿”å›åŒºåŸŸç»“æœ")
    for img in tqdm(images, file=sys.stdout, desc=" - é¢éƒ¨æ£€æµ‹å¼€å§‹å¤„ç†", ascii="â–‘â–’â–ˆ"):
        detected.append(detect_faces(img, level, version, max_infer_size, conf_threshold, iou_threshold))
    output_cache = detected
    return detected


def head_detect(dataset_name, level, max_infer_size, conf_threshold, iou_threshold):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    detected = []
    if level:
        level = "s"
    else:
        level = "n"
    # print(" - å¤´éƒ¨æ£€æµ‹å¼€å§‹å¤„ç†")
    # print("   *å°†è¿”å›åŒºåŸŸç»“æœ")
    for img in tqdm(images, file=sys.stdout, ascii="â–‘â–’â–ˆ", desc=" - å¤´éƒ¨æ£€æµ‹å¼€å§‹å¤„ç†"):
        detected.append(detect_heads(img, level, max_infer_size, conf_threshold, iou_threshold))
    output_cache = detected
    return detected


def area_fill(dataset_name, is_random, color):
    global output_cache
    area = output_cache
    images = dataset_getImg(dataset_name)[0]
    fill = []
    # print(" - åŒºåŸŸå¡«å……å¼€å§‹å¤„ç†")
    for img, xyxy in tzip(images, area, file=sys.stdout, ascii="â–‘â–’â–ˆ", desc=" - åŒºåŸŸå¡«å……å¼€å§‹å¤„ç†"):
        xyxy = xyxy[0]
        if is_random:
            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
        # print(img, eval(area), xyxy, xyxy[0][0])
        fill.append(censor_areas(img, 'color', [xyxy[0]], color=color))  # [xyxy[0][0]]
    # os.makedirs(f"processed/{dataset_name}", exist_ok=True)
    # for i, sv in enumerate(fill):
    #     sv.save(f"processed/{dataset_name}/{dataset_name}_AreaFill_{i+1}.png")
    output_cache = fill
    return fill


def crop_hw(dataset_name):
    global output_cache
    mask_info = output_cache
    images = dataset_getImg(dataset_name)[0]
    result = []
    for img, infos in zip(images, mask_info):
        infos = infos[0]
        (x0, y0, x1, y1) = infos[0]
        detect_type = infos[1]
        # score = infos[2]
        img_array = numpy.array(rgb_encode(img))
        h, w = img_array.shape[1:]
        mask = numpy.zeros((h, w))
        # print(mask, h, w)
        if detect_type == 'face' or detect_type == 'head':
            mask[y0:y1, x0:x1] = 1
            # print(mask)
        else:
            output_cache = []
            return "æ­¤å†…å®¹ä¸æ”¯æŒå‰ªè£"
        result.append(squeeze(img, mask))
    output_cache = result
    return result


def crop_trans(dataset_name, threshold, filter_size):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    out = []
    # print(" - è‡ªé€‚åº”è£å‰ªå¼€å§‹å¤„ç†")
    for img in tqdm(images, file=sys.stdout, desc=" - è‡ªé€‚åº”è£å‰ªå¼€å§‹å¤„ç†", ascii="â–‘â–’â–ˆ"):
        if img is not None:
            out.append(squeeze_with_transparency(img, threshold, filter_size))
    output_cache = out
    return out


def img_segment(dataset_name, scale):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    out = []
    # print(" - äººç‰©åˆ†ç¦»å¼€å§‹å¤„ç†")
    for img in tqdm(images, file=sys.stdout, desc=" - äººç‰©åˆ†ç¦»å¼€å§‹å¤„ç†", ascii="â–‘â–’â–ˆ"):
        out.append(segment_rgba_with_isnetis(img, scale)[1])  # maskä¿¡æ¯è¢«ä¸¢å¼ƒäº†
    output_cache = out
    return out


def ref_datasets(need_list=False):
    # éå†æœ¬åœ°æ•°æ®é›†
    list_datasets = []
    with os.scandir("dataset") as datasets:
        for each_dataset in datasets:
            # f_dataset = each_dataset.__next__()
            if not each_dataset.name.startswith('.') and each_dataset.is_dir():
                list_datasets.append(each_dataset.name)
    if need_list:
        return list_datasets
    else:
        return gr.Dropdown.update(choices=list_datasets)


def ref_customList(need_list=False):
    custom_blacklist = []
    with os.scandir("cfgs/blacklist") as blacklists:
        for each_black in blacklists:
            if not each_black.name.startswith('.') and each_black.is_file():
                if each_black.name.endswith('.txt') or each_black.name.endswith('.json'):
                    custom_blacklist.append(each_black.name)
    if need_list:
        return custom_blacklist
    else:
        return gr.Dropdown.update(choices=custom_blacklist)


def ref_runs(dataset_name, need_list=False):
    runs_list = []
    try:
        with os.scandir(f"runs/{dataset_name}/ckpts") as conv_list:
            for conv in conv_list:
                # print("éå†äº†ä¸€ä¸ªconv")
                if conv.is_file():
                    if conv.name.endswith('.pt'):
                        runs_list.append(conv.name.replace(dataset_name+"-", "").replace(".pt", ""))
    except FileNotFoundError:
        if need_list:
            return []
        else:
            return gr.Dropdown.update(choices=[])
    if runs_list is not None:
        runs_list = sorted(runs_list, key=int, reverse=True)
        if need_list:
            return runs_list
        else:
            # print("ç»“æœ"+str(runs_list))
            return gr.Dropdown.update(choices=runs_list)


def convert_weights(dataset_name, step):
    global output_cache
    # logging.try_init_root(logging.INFO)
    convert_to_webui_lora(f"runs/{dataset_name}/ckpts/unet-{step}.safetensors",
                          f"runs/{dataset_name}/ckpts/text_encoder-{step}.safetensors",
                          os.path.join(f"runs/{dataset_name}/ckpts", f"{dataset_name}-lora-{step}.safetensors")
                          )
    output_cache = []
    return "å·²æ‰§è¡Œè½¬æ¢"


def tagger_chooser_ctrl(evt: gr.SelectData):  # æ­¤æ–¹æ³•ä½¿ç”¨å…¨å±€å˜é‡
    # print(evt.value+"æ­£åœ¨é€‰æ‹©")
    # éšè—æ‰€æœ‰æ‰“æ ‡å™¨è®¾ç½®
    updates = {}
    for tagger in taggers:
        # if tagger == "æ ‡ç­¾é»‘åå•":
        #     tagger = "dropper"
        if tagger == "jsonè§£æ":
            tagger = "anal"
        updates[globals()[f"tagger_{tagger}_settings"]] = gr.update(visible=False)
    # æ˜¾ç¤ºæ‰“æ ‡å™¨è®¾ç½®
    if evt.value in taggers:
        # if evt.value == "æ ‡ç­¾é»‘åå•":
        #     evt.value = "dropper"
        if evt.value == "jsonè§£æ":
            evt.value = "anal"
        updates[globals()[f"tagger_{evt.value}_settings"]] = gr.update(visible=True)
        # print(evt.value, tagger_dropper_settings.visible)
    # updates[globals()[f"wd14_use_blacklist"]] = gr.update(value=wd14_use_blacklist.value)
    # updates[globals()[f"ml_use_blacklist"]] = gr.update(value=ml_use_blacklist.value)
    # updates[globals()[f"tagger_dropper_settings"]] = gr.update(visible=tagger_dropper_settings.visible)
    return updates


def blacklist_settings_ctrl(evt: gr.SelectData):
    updates = {}
    if evt.selected:
        updates[tagger_dropper_settings] = gr.update(visible=True)
    else:
        updates[tagger_dropper_settings] = gr.update(visible=False)
    return updates


def custom_blacklist_ctrl(evt: gr.SelectData):
    if evt.selected:
        dropper_update = {globals()[f"drop_custom_setting"]: gr.update(visible=False)}
    else:
        dropper_update = {globals()[f"drop_custom_setting"]: gr.update(visible=True)}
    return dropper_update


def pixiv_setting_ctrl(evt: gr.SelectData):
    if evt.index == 1:
        update = {globals()[f"pixiv_settings"]: gr.update(visible=True)}
    else:
        update = {globals()[f"pixiv_settings"]: gr.update(visible=False)}
    return update


def color_picker_ctrl(evt: gr.SelectData):
    if evt.selected:
        update = {globals()[f"areaf_color"]: gr.update(visible=False)}
    else:
        update = {globals()[f"areaf_color"]: gr.update(visible=True)}
    return update


def save_output_ctrl():
    global output_cache
    if has_image(output_cache):
        update = {globals()[f"save_output"]: gr.update(interactive=True)}
    else:
        update = {globals()[f"save_output"]: gr.update(interactive=False)}
    return update


def saving_output(dataset_name):
    global output_cache
    count = 0
    if dataset_name.endswith("_processed"):
        process_dir = f"dataset/{dataset_name}"
    else:
        process_dir = f"dataset/{dataset_name}_processed"
    if has_image(output_cache):
        os.makedirs(process_dir, exist_ok=True)
        anyfiles = os.listdir(process_dir)
        # print(" - å¼€å§‹ä¿å­˜è¿è¡Œç»“æœ")
        for anyfile in anyfiles:
            os.remove(f"{process_dir}/{anyfile}")
        for i, sv in enumerate(tqdm(output_cache, file=sys.stdout, desc=" - å¼€å§‹ä¿å­˜è¿è¡Œç»“æœ", ascii="â–‘â–’â–ˆ")):
            sv.save(f"{process_dir}/{dataset_name}_{i+1}.png")
            count = count+1
        output_cache = []
        return "å·²ä¿å­˜ "+str(count)+" å¼ å›¾ç‰‡è‡³"+process_dir+"æ–‡ä»¶å¤¹"


def tagging_main(dataset_name, ttype, wd14_tagger, wd14_general_thre, wd14_character_thre, wd14_weight, wd14_overlap, ml_real_name, ml_thre, ml_scale, ml_weight, ml_ratio, ml_overlap, need_black, drop_presets, drop_custom, exists_txt, del_json):
    global output_cache
    images = dataset_getImg(dataset_name)[0]
    img_name = images[1]
    result = []
    if ttype == taggers[0]:
        # print(" - æ•°æ®æ‰“æ ‡å¼€å§‹å¤„ç†")
        for img, name in tzip(images, img_name, file=sys.stdout, ascii="â–‘â–’â–ˆ", desc=" - æ•°æ®æ‰“æ ‡å¼€å§‹å¤„ç†"):
            result = get_wd14_tags(img, wd14_tagger, wd14_general_thre, wd14_character_thre, wd14_overlap)
            if result[2]:
                result = tags_to_text(result[1], include_score=wd14_weight)+', '+tags_to_text(result[2], include_score=wd14_weight)  # features and chars
            if need_black:
                result = str(str(drop_blacklisted_tags([result], drop_presets, drop_custom))[2:-2])
            if result:
                name = name.replace(".txt", "")
                if os.path.isfile(f'dataset/{dataset_name}/{name}.txt'):
                    if exists_txt == "å¤åˆ¶æ–‡ä»¶":
                        os.rename(f'dataset/{dataset_name}/{name}.txt', f'{dataset_name}/{name}_backup.txt')
                        with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                            tag.write(result)
                    elif exists_txt == "å¿½ç•¥æ–‡ä»¶":
                        pass
                    elif exists_txt == "é™„åŠ æ ‡ç­¾":
                        with open(f'dataset/{dataset_name}/{name}.txt', 'a+') as tag:
                            tag.write(result)
                    elif exists_txt == "è¦†ç›–æ–‡ä»¶":
                        with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                            tag.write(result)
                else:
                    with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                        tag.write(result)
    elif ttype == taggers[1]:
        # print(" - æ•°æ®æ‰“æ ‡å¼€å§‹å¤„ç†")
        for img, name in tzip(images, img_name, file=sys.stdout, ascii="â–‘â–’â–ˆ", desc=" - æ•°æ®æ‰“æ ‡å¼€å§‹å¤„ç†"):
            result = get_mldanbooru_tags(img, ml_real_name, ml_thre, ml_scale, ml_ratio, ml_overlap)
            result = tags_to_text(result, include_score=ml_weight)
            if need_black:
                result = str(str(drop_blacklisted_tags([result], drop_presets, drop_custom))[2:-2])
            # print(result)
            if result:
                name = name.replace(".txt", "")
                if os.path.isfile(f'dataset/{dataset_name}/{name}.txt'):
                    if exists_txt == "å¤åˆ¶æ–‡ä»¶":
                        os.rename(f'dataset/{dataset_name}/{name}.txt', f'{dataset_name}/{name}_backup.txt')
                        with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                            tag.write(result)
                    elif exists_txt == "å¿½ç•¥æ–‡ä»¶":
                        pass
                    elif exists_txt == "é™„åŠ æ ‡ç­¾":
                        with open(f'dataset/{dataset_name}/{name}.txt', 'a+') as tag:
                            tag.write(result)
                    elif exists_txt == "è¦†ç›–æ–‡ä»¶":
                        with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                            tag.write(result)
                else:
                    with open(f'dataset/{dataset_name}/{name}.txt', 'w') as tag:
                        tag.write(result)
    elif ttype == taggers[2]:
        json_files = glob.glob(f'dataset/{dataset_name}/.*.json')
        # print(" - æ ‡ç­¾è§£æå¼€å§‹å¤„ç†")
        for json_file in tqdm(json_files, file=sys.stdout, desc=" - æ ‡ç­¾è§£æå¼€å§‹å¤„ç†", ascii="â–‘â–’â–ˆ"):
            with open(json_file, 'r') as f:
                jdata = json.load(f)
            danbooru_data = jdata.get('danbooru', {})
            tag_json_general = danbooru_data.get('tag_string_general', '')
            tag_json_character = danbooru_data.get('tag_string_character, ')
            if tag_json_general:
                tag_json_general = tag_json_general.replace(' ', ', ')
            if tag_json_character:
                tag_json_character = tag_json_character.replace(' ', ', ')
            if tag_json_general is None and tag_json_character is None:
                output_cache = []
                return "æ— æ ‡ç­¾"
            elif tag_json_general is None:
                tag_json = tag_json_character
            elif tag_json_character is None:
                tag_json = tag_json_general
            else:
                tag_json = f"{tag_json_general}\n{tag_json_character}\n"
            if need_black:
                tag_json = str(str(drop_blacklisted_tags([tag_json], drop_presets, drop_custom))[2:-2])
            txtfile_name = json_file.replace('.', '', 1).replace('_meta.json', '.txt')
            if tag_json_general or tag_json_character:
                if os.path.isfile(f'{txtfile_name}'):
                    if exists_txt == "å¤åˆ¶æ–‡ä»¶":
                        os.rename(f'{txtfile_name}', f'{txtfile_name}'.replace('.txt', '_backup.txt'))
                        with open(f'{txtfile_name}', 'w') as f:
                            f.write(tag_json)
                    elif exists_txt == "å¿½ç•¥æ–‡ä»¶":
                        pass
                    elif exists_txt == "é™„åŠ æ ‡ç­¾":
                        with open(f'{txtfile_name}', 'a+') as f:
                            f.write(tag_json)
                    elif exists_txt == "è¦†ç›–æ–‡ä»¶":
                        with open(f'{txtfile_name}', 'w') as f:
                            f.write(tag_json)
                else:
                    with open(f'{txtfile_name}', 'w') as f:
                        f.write(tag_json)
                if del_json:
                    os.remove(json_file)


# @gr.StateHandler
def pre_rating_limit(rating):
    updates = {}
    # print(evt.index)
    # print(pre_rating)
    # print(rating)
    if not rating:
        updates[download_button] = gr.update(interactive=False)
    else:
        updates[download_button] = gr.update(interactive=True)
    return updates


# ä¸»ç•Œé¢
with gr.Blocks(css="style.css", analytics_enabled=False) as iblock:
    output_cache = []
    quicksettings = gr.Row(elem_id="quicksettings")
    with quicksettings:
        dataset_dropdown = gr.Dropdown(ref_datasets(True), label="å½“å‰æ•°æ®é›†", value=ref_datasets(True)[0], container=True, show_label=True, interactive=True, elem_id='dataset_dropbar')
        ref_datasets_button = gr.Button("ğŸ”„", elem_id='refresh_datasets')
    with gr.Tab("æ•°æ®é›†è·å–"):
        source = gr.Radio(['Danbooru', 'Pixiv', 'Zerochan'], label='é€‰æ‹©å›¾ç«™', value='Danbooru')
        char_name = gr.Textbox(label='è§’è‰²åç§°', value='', placeholder='å¡«å…¥è§’è‰²å')
        pre_min_size = gr.Textbox(label="æœ€å°å°ºå¯¸", value="600", interactive=False)
        pre_background = gr.ColorPicker(label="èƒŒæ™¯è‰²", value="#FFFFFF", interactive=False)
        pre_class = gr.CheckboxGroup(["ç´ æ", "æ¼«ç”»", "3D"], label="é£æ ¼è¿‡æ»¤", value=None, type="index", interactive=False)
        pre_rating = gr.CheckboxGroup(["å¥å…¨", "r15", "r18"], label="è¯„çº§è¿‡æ»¤", value=["å¥å…¨"], type="index", interactive=False)
        pre_crop_person = gr.Checkbox(label="è£å‰ªäººç‰©", value=False, interactive=False)
        pre_auto_tagging = gr.Checkbox(label="è‡ªåŠ¨æ‰“æ ‡", value=False, interactive=False)
        with gr.Column(visible=False) as pixiv_settings:
            pixiv_no_ai = gr.Checkbox(label="éAIç”Ÿæˆ", interactive=True, value=False)
            pixiv_token = gr.Textbox(label="åˆ·æ–°ä»¤ç‰Œ", placeholder="å¿…é¡»: pixivçš„refresh token", interactive=True)
            pixiv_get_token = gr.Button("å‰å¾€æŸ¥è¯¢", interactive=True)
            with gr.Accordion("ä»¤ç‰Œè¯´æ˜", open=False):
                gr.Markdown("è·å–Pixivå›¾ç‰‡éœ€è¦åˆ·æ–°ä»¤ç‰Œ\n"
                            "ç”¨æ³•ï¼šç‚¹å‡»`å‰å¾€è·å–`ï¼Œå°†æ‰“å¼€Pixivç½‘é¡µï¼ŒæŒ‰F12å¯ç”¨å¼€å‘è€…æ§åˆ¶å°ï¼Œé€‰æ‹©`ç½‘ç»œ/Network`ï¼Œç‚¹å‡»å·¦ä¾§ç¬¬ä¸‰ä¸ªæŒ‰é’®`ç­›é€‰å™¨`ï¼Œ"
                            "ç­›é€‰`callback?`ç‚¹å‡»ç»§ç»­ä½¿ç”¨æ­¤è´¦å·ç™»é™†ï¼Œæ­¤æ—¶é¡µé¢ä¼šè·³è½¬ï¼Œå¼€å‘è€…æ§åˆ¶å°ä¼šå‡ºç°ä¸€æ¡è¯·æ±‚ï¼Œç‚¹å‡»å®ƒï¼Œè¿›å…¥`æ ‡å¤´`"
                            "å¤åˆ¶`code=`åçš„å†…å®¹ï¼Œå¡«å…¥åå°ï¼ˆé»‘çª—å£ï¼‰æŒ‰å›è½¦ï¼Œåå°å°†è¿”å›ä½ çš„refresh token\n"
                            "å–æ¶ˆè·å–è¯·æŒ‰ctrl+c")
        source.select(pixiv_setting_ctrl, None, [pixiv_settings])
        dl_count = gr.Slider(1, 100, label='ä¸‹è½½æ•°é‡', value=10, step=1)
        # save_path = gr.Textbox(label='ä¿å­˜è·¯å¾„', value='dataset', placeholder='è‡ªåŠ¨åˆ›å»ºå­æ–‡ä»¶å¤¹')
        download_button = gr.Button("è·å–å›¾ç‰‡", variant="primary", interactive=True)
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("æ•°æ®é›†è¯¦ç»†è¯´æ˜..ä»€ä¹ˆçš„")
        pre_rating.change(pre_rating_limit, [pre_rating], [download_button])
        pixiv_get_token.click(get_ref_token, [], [])
    with gr.Tab("æ•°æ®é›†å¤„ç†"):
        with gr.Accordion("ä¸‰é˜¶åˆ†å‰²"):
            stage_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
        with gr.Accordion("å·®åˆ†è¿‡æ»¤"):
            cluster_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("å·®åˆ†æ£€æµ‹ï¼šLPIPSï¼ˆæ„ŸçŸ¥å›¾åƒè¡¥ä¸ç›¸ä¼¼æ€§ï¼‰ ï¼Œå…¨ç§°ä¸ºLearned Perceptual Image Patch "
                            "Similarityï¼Œæ˜¯ä¸€ç§ç”¨äºè¯„ä¼°å›¾åƒç›¸ä¼¼æ€§çš„åº¦é‡æ–¹æ³•ã€‚åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡æ¯”è¾ƒå›¾åƒä¹‹é—´çš„æ·±åº¦ç‰¹å¾è¯„ä¼°å®ƒä»¬çš„ç›¸ä¼¼æ€§\n "
                            "LPIPSä½¿ç”¨äº†é¢„è®­ç»ƒçš„åˆ†ç±»ç½‘ç»œï¼ˆå¦‚AlexNetæˆ–VGGï¼‰æ¥æå–å›¾åƒçš„ç‰¹å¾ã€‚ç„¶åè®¡ç®—ä¸¤ä¸ªå›¾åƒç‰¹å¾ä¹‹é—´çš„ä½™å¼¦è·ç¦»ï¼Œ"
                            "å¹¶å¯¹æ‰€æœ‰å±‚å’Œç©ºé—´ç»´åº¦çš„è·ç¦»è¿›è¡Œå¹³å‡ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªå€¼ï¼Œç”¨äºè¡¨ç¤ºä¸¤ä¸ªå›¾åƒä¹‹é—´çš„æ„ŸçŸ¥å·®å¼‚ã€‚\n"
                            "*ä¼šè¿”å›å»é™¤å·®åˆ†åçš„å›¾ç‰‡ç»“æœ"
                            "![cluster](file/markdown_res/lpips_full.plot.py.svg)")
        with gr.Accordion("äººç‰©åˆ†ç¦»"):
            seg_scale = gr.Slider(32, 2048,label="ç¼©æ”¾å¤§å°", info="å›¾åƒä¼ é€’ç»™æ¨¡å‹æ—¶çš„ç¼©æ”¾å°ºå¯¸", step=32, value=1024, interactive=True)
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("äººç‰©åˆ†ç¦»\n"
                            "*ä¼šè¿”å›èƒŒæ™¯ä¸ºé€æ˜çš„äººç‰©å›¾ç‰‡ç»“æœ\n"
                            "æŸ¥é˜…skytntçš„[å¤æ‚åŠ¨æ¼«æŠ åƒ](https://github.com/SkyTNT/anime-segmentation/)")
            seg_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
        # with gr.Accordion("äººç‰©æ£€æµ‹"):
        #     ccip_level = gr.Checkbox(label="ä½¿ç”¨é«˜ç²¾åº¦", value=True, interactive=True)
        #     ccip_model = gr.Dropdown(["v0", "v1", "v1.1"], label="æ¨¡å‹é€‰æ‹©", value="v1.1", interactive=True)
        #     ccip_infer = gr.Slider(32, 2048, label="ç¼©æ”¾å¤§å°", interactive=True, step=32, value=640, info="å›¾åƒä¼ é€’ç»™æ¨¡å‹æ—¶çš„ç¼©æ”¾å°ºå¯¸")
        #     ccip_conf = gr.Slider(0.01, 1, label="æ£€æµ‹é˜ˆå€¼", interactive=True, value=0.25, step=0.01, info="ç½®ä¿¡åº¦é«˜äºæ­¤å€¼çš„æ£€æµ‹ç»“æœä¼šè¢«è¿”å›")
        #     ccip_iou = gr.Slider(0.01, 1, label="é‡å é˜ˆå€¼", interactive=True, value=0.7, step=0.01, info="é‡å åŒºåŸŸé«˜äºæ­¤é˜ˆå€¼å°†ä¼šè¢«ä¸¢å¼ƒ")
        #     ccip_button = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
        #     with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
        #         gr.Markdown("è§’è‰²æ£€æµ‹ï¼šCCIPï¼ˆå¯¹æ¯”è§’è‰²å›¾åƒé¢„è®­ç»ƒï¼‰ä»åŠ¨æ¼«è§’è‰²å›¾åƒä¸­æå–ç‰¹å¾ï¼Œè®¡ç®—ä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„è§†è§‰å·®å¼‚ï¼Œå¹¶ç¡®å®šä¸¤ä¸ªå›¾åƒæ˜¯å¦"
        #                     "æç»˜ç›¸åŒçš„è§’è‰²ã€‚![ccip](file/markdown_res/ccip_full.plot.py.svg)"
        #                     "æ›´å¤šä¿¡æ¯å¯æŸ¥é˜… [CCIPå®˜æ–¹æ–‡æ¡£](https://deepghs.github.io/imgutils/main/api_doc/metrics/ccip.html).")
        with gr.Accordion("é¢éƒ¨æ£€æµ‹"):
            faced_level = gr.Checkbox(value=True, label="ä½¿ç”¨é«˜ç²¾åº¦", interactive=True)
            faced_model = gr.Dropdown(["v0", "v1", "v1.3", "v1.4"], label="æ¨¡å‹é€‰æ‹©", value="v1.4", interactive=True)
            faced_infer = gr.Slider(32,2048, label="ç¼©æ”¾å¤§å°", interactive=True, step=32, value=640, info="å›¾åƒä¼ é€’ç»™æ¨¡å‹æ—¶çš„ç¼©æ”¾å°ºå¯¸")
            faced_conf = gr.Slider(0.01, 1, label="æ£€æµ‹é˜ˆå€¼", interactive=True, value=0.25, step= 0.01, info="ç½®ä¿¡åº¦é«˜äºæ­¤å€¼çš„æ£€æµ‹ç»“æœä¼šè¢«è¿”å›")
            faced_iou = gr.Slider(0.01, 1, label="é‡å é˜ˆå€¼", interactive=True, value=0.7, step=0.01, info="é‡å åŒºåŸŸé«˜äºæ­¤é˜ˆå€¼å°†ä¼šè¢«ä¸¢å¼ƒ")
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("##é¢éƒ¨æ£€æµ‹"
                            "æ¥è‡ªimgutilsæ£€æµ‹æ¨¡å—"
                            "###æ­¤åŠŸèƒ½ä¼šè¿”å›ä¸€ä¸ªåŒºåŸŸç»“æœï¼Œè€Œä¸æ˜¯å›¾ç‰‡ç»“æœ")
            faced_button = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
        with gr.Accordion("å¤´éƒ¨æ£€æµ‹"):
            headd_level = gr.Checkbox(value=True, label="ä½¿ç”¨é«˜ç²¾åº¦", interactive=True)
            headd_infer = gr.Slider(32,2048, label="ç¼©æ”¾å¤§å°", interactive=True, step=32, value=640, info="å›¾åƒä¼ é€’ç»™æ¨¡å‹æ—¶çš„ç¼©æ”¾å°ºå¯¸")
            headd_conf = gr.Slider(0.01, 1, label="æ£€æµ‹é˜ˆå€¼", interactive=True, value=0.25, step=0.01, info="ç½®ä¿¡åº¦é«˜äºæ­¤å€¼çš„æ£€æµ‹ç»“æœä¼šè¢«è¿”å›")
            headd_iou = gr.Slider(0.01, 1, label="é‡å é˜ˆå€¼", interactive=True, value=0.7, step=0.01, info="é‡å åŒºåŸŸé«˜äºæ­¤é˜ˆå€¼å°†ä¼šè¢«ä¸¢å¼ƒ")
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("##å¤´éƒ¨æ£€æµ‹"
                            "æ¥è‡ªimgutilsæ£€æµ‹æ¨¡å—"
                            "###æ­¤åŠŸèƒ½ä¼šè¿”å›ä¸€ä¸ªåŒºåŸŸç»“æœï¼Œè€Œä¸æ˜¯å›¾ç‰‡ç»“æœ")
            headd_button = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
        with gr.Accordion("åŒºåŸŸå¡«å……"):
            areaf_isRandom = gr.Checkbox(label="éšæœºé¢œè‰²", value=True, interactive=True)
            areaf_color = gr.ColorPicker(label="è‡ªå®šä¹‰é¢œè‰²", value="#00FF00", visible=not areaf_isRandom.value)
            areaf_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("æ¥æ”¶è¾“å‡ºåçš„ç»“æœè¿›è¡Œæ‰“ç ã€‚\n"
                            "è¿è¡Œç»“æœå†…æœ‰åŒºåŸŸä¿¡æ¯ï¼Œæ‰å¯ä»¥å¡«å……...")
            areaf_isRandom.select(color_picker_ctrl, None, [areaf_color])
        with gr.Accordion("åŒºåŸŸå‰ªè£"):
            crop_hw_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("å°†è¿è¡Œç»“æœä¸­çš„åŒºåŸŸè¿›è¡Œå‰ªè£ã€‚\n"
                            "è¿è¡Œç»“æœå†…æœ‰åŒºåŸŸä¿¡æ¯ï¼Œæ‰å¯ä»¥å‰ªè£...")
        with gr.Accordion("è‡ªé€‚åº”å‰ªè£"):
            crop_trans_button = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            crop_trans_thre = gr.Slider(0.01, 1, label="å®¹å·®é˜ˆå€¼", value=0.7, step=0.01)
            crop_trans_filter = gr.Slider(0, 10, label="ç¾½åŒ–", value=5, step=1)
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("å°†æ•°æ®é›†ä¸­çš„é€æ˜å›¾ç‰‡è¿›è¡Œè‡ªé€‚åº”å‰ªè£ã€‚\n"
                            "ä¸å¯¹è¿è¡Œç»“æœä¸­çš„å†…å®¹è¿›è¡Œæ“ä½œã€‚")
    with gr.Tab("æ‰“æ ‡å™¨"):
        taggers = ["wd14", "mldanbooru", "jsonè§£æ"]
        tagger_type = gr.Dropdown(taggers, value=taggers[0], label="æ‰“æ ‡å™¨", allow_custom_value=False, interactive=True)
        with gr.Column(visible=tagger_type.value == taggers[0]) as tagger_wd14_settings:
            wd14_tagger_model = gr.Dropdown(["SwinV2", "ConvNext", "ConvNextV2", "ViT", "MOAT"], value="ConvNextV2", label="æ‰“æ ‡æ¨¡å‹", interactive=True)
            wd14_general_threshold = gr.Slider(0.01, 1, value=0.35, label="æ™®é€šæ ‡ç­¾é˜ˆå€¼", step=0.01, interactive=True)
            wd14_character_threshold = gr.Slider(0.01, 1, value=0.85, label="è§’è‰²æ ‡ç­¾é˜ˆå€¼", step=0.01, interactive=True)
            wd14_format_weight = gr.Checkbox(label="å†™å…¥æƒé‡", value=False, interactive=True)
            wd14_drop_overlap = gr.Checkbox(value=True, label="ç²¾ç¡®æ‰“æ ‡", interactive=True)
            # wd14_use_blacklist = gr.Checkbox(label="ä½¿ç”¨é»‘åå•", value=True, interactive=True)
        with gr.Column(visible=tagger_type.value == taggers[1]) as tagger_mldanbooru_settings:
            ml_use_real_name = gr.Checkbox(value=False, label="æ ‡ç­¾é‡å®šå‘", info="ç”±äºåœ¨Deepdanbooruè®­ç»ƒåï¼ŒDanbooruç½‘ç«™ä¸Šçš„è®¸å¤šæ ‡ç­¾éœ€è¦é‡å‘½åå’Œé‡å®šå‘ï¼Œå› æ­¤åœ¨æŸäº›åº”ç”¨åœºæ™¯ä¸­å¯èƒ½æœ‰å¿…è¦ä½¿ç”¨æœ€æ–°çš„æ ‡ç­¾åç§°ã€‚")
            ml_threshold = gr.Slider(0.01, 1, value=0.7, label="æ ‡ç­¾é˜ˆå€¼", step=0.01, interactive=True)
            ml_size = gr.Slider(32, 1024, value=448, step=32, label="ç¼©æ”¾å¤§å°", interactive=True, info="å°†ç¼©æ”¾åçš„å›¾åƒä¼ é€’ç»™æ¨¡å‹æ—¶çš„å¤§å°")
            ml_keep_ratio = gr.Checkbox(value=False, label="ä¿æŒæ¯”ä¾‹", info="ä¿æŒè®­ç»ƒé›†å›¾åƒçš„åŸå§‹æ¯”ä¾‹", interactive=True)
            ml_format_weight = gr.Checkbox(label="å†™å…¥æƒé‡", value=False, interactive=True)
            ml_drop_overlap = gr.Checkbox(value=True, label="ç²¾ç¡®æ‰“æ ‡", interactive=True)
            # ml_use_blacklist = gr.Checkbox(label="ä½¿ç”¨é»‘åå•", value=True, interactive=True)
        with gr.Column(visible=tagger_type.value == taggers[2]) as tagger_anal_settings:
            with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown("ç”¨æ­¤è„šæœ¬è·å–çš„å›¾ç‰‡é™„æœ‰jsonæ–‡ä»¶\n"
                            "ä½¿ç”¨æ­¤æ‰“æ ‡å™¨ä»¥ä»ä¸­æå–tag\n"
                            "æ­¤åŠŸèƒ½ä¸ä¼šæ£€æŸ¥å›¾ç‰‡ï¼Œè€Œæ˜¯ä»æ‰€æœ‰å¯èƒ½çš„jsonæ–‡ä»¶ä¸­æå–tag")
            anal_del_json = gr.Checkbox(value=False, label="åˆ é™¤json", interactive=True)
        use_blacklist = gr.Checkbox(label="ä½¿ç”¨é»‘åå•", value=True, interactive=True)
        with gr.Column(visible=use_blacklist.value) as tagger_dropper_settings:
            drop_use_presets = gr.Checkbox(value=True, label="ä½¿ç”¨åœ¨çº¿é»‘åå•", info="è·å–åœ¨çº¿é»‘åå•ï¼Œæ¥è‡ªalea31435", interactive=True)
            with gr.Column(visible=not drop_use_presets.value, elem_id="drop_custom_setting") as drop_custom_setting:
                drop_custom_list = gr.Dropdown(ref_customList(True), value=ref_customList(True)[0], label="è‡ªå®šä¹‰é»‘åå•", elem_id="custom_list", interactive=True, info="é»‘åå•è·¯å¾„cfgs/blacklist/")
                drop_ref_button = gr.Button("ğŸ”„", elem_id='refresh_custom_list')
        op_exists_txt = gr.Dropdown(["å¤åˆ¶æ–‡ä»¶", "å¿½ç•¥æ–‡ä»¶", "è¦†ç›–æ–‡ä»¶", "é™„åŠ æ ‡ç­¾"], value="é™„åŠ æ ‡ç­¾", info="å¯¹äºå·²å­˜åœ¨æ ‡ç­¾ï¼Œæ‰“æ ‡å™¨çš„è¡Œä¸º", show_label=False, interactive=True)
        tagger_button = gr.Button("æ‰“æ ‡", variant="primary")
        # tagger_type.select(tagger_chooser_ctrl, None, [globals()[f'tagger_{("dropper" if tagger == "æ ‡ç­¾é»‘åå•" else tagger)}_settings'] for tagger in taggers])
        tagger_type.select(tagger_chooser_ctrl, None, [globals()[f'tagger_{("anal" if tagger == "jsonè§£æ" else tagger)}_settings'] for tagger in taggers])
        # wd14_use_blacklist.select(blacklist_settings_ctrl, None, [tagger_dropper_settings])
        # ml_use_blacklist.select(blacklist_settings_ctrl, None, [tagger_dropper_settings])
        use_blacklist.select(blacklist_settings_ctrl, None, [tagger_dropper_settings])
        drop_use_presets.select(custom_blacklist_ctrl, None, [drop_custom_setting])
    with gr.Tab("PLoRAè®­ç»ƒ"):
        min_step = gr.Textbox(label="æœ€å°æ­¥æ•°", value='', placeholder='ä¸å¡«å†™å°†è‡ªåŠ¨è®¡ç®—')
        train_button = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary")
        with gr.Accordion("æƒé‡åˆå¹¶", open=True):
            with gr.Column(elem_id="convert_lora_steps") as convert_lora_steps:
                convert_step = gr.Dropdown(ref_runs(dataset_dropdown.value, True), value=ref_runs(dataset_dropdown.value, True)[0] if ref_runs(dataset_dropdown.value, True) else [], label="æ­¥æ•°", info="åˆå¹¶å¯¹åº”æ­¥æ•°çš„æƒé‡æ–‡ä»¶", elem_id="convert_list", multiselect=False, interactive=True)
                convert_ref_button = gr.Button("ğŸ”„", elem_id='convert_ref_button')
            convert_weights_button = gr.Button("å¼€å§‹åˆå¹¶", variant="primary")
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("è®­ç»ƒè¯¦ç»†è¯´æ˜..ä»€ä¹ˆçš„")
    with gr.Tab("è´¨é‡éªŒè¯"):
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("soon...")
    with gr.Column(elem_id="output"):
        message_output = gr.Textbox(label='è¿è¡Œç»“æœ', elem_id="message_output")
        save_output = gr.Button("ğŸ’¾", elem_id="save_output", interactive=False)
        message_output.change(save_output_ctrl, [], save_output)
    download_button.click(download_images, [source, char_name, pre_min_size, pre_background, pre_class, pre_rating, pre_crop_person, pre_auto_tagging, dl_count, pixiv_token, pixiv_no_ai], [message_output], scroll_to_output=True)
    ref_datasets_button.click(ref_datasets, [], [dataset_dropdown])
    stage_button.click(three_stage, [dataset_dropdown], [message_output])
    drop_ref_button.click(ref_customList, [], [drop_custom_list])
    convert_ref_button.click(ref_runs, [dataset_dropdown], [convert_step])
    convert_weights_button.click(convert_weights, [dataset_dropdown, convert_step], [message_output])
    cluster_button.click(clustering, [dataset_dropdown], [message_output], scroll_to_output=True)
    seg_button.click(img_segment, [dataset_dropdown, seg_scale], [message_output], scroll_to_output=True)
    # ccip_button.click(person_detect, [dataset_dropdown, ccip_level, ccip_model, ccip_infer, ccip_conf, ccip_iou], [message_output])
    faced_button.click(face_detect, [dataset_dropdown, faced_level, faced_model, faced_infer, faced_conf, faced_iou], [message_output], scroll_to_output=True)
    headd_button.click(head_detect, [dataset_dropdown, headd_level, headd_infer, headd_conf, headd_iou], [message_output], scroll_to_output=True)
    train_button.click(run_train_plora, [dataset_dropdown, dataset_dropdown, min_step], [message_output], scroll_to_output=True)
    areaf_button.click(area_fill, [dataset_dropdown, areaf_isRandom, areaf_color], [message_output], scroll_to_output=True)
    crop_hw_button.click(crop_hw, [dataset_dropdown], [message_output], scroll_to_output=True)
    crop_trans_button.click(crop_trans, [dataset_dropdown, crop_trans_thre, crop_trans_filter], [message_output], scroll_to_output=True)
    tagger_button.click(tagging_main, [dataset_dropdown, tagger_type, wd14_tagger_model, wd14_general_threshold, wd14_character_threshold, wd14_format_weight, wd14_drop_overlap, ml_use_real_name, ml_threshold, ml_size, ml_format_weight, ml_keep_ratio, ml_drop_overlap, use_blacklist, drop_use_presets, drop_custom_list, op_exists_txt, anal_del_json], [message_output], scroll_to_output=True)
    save_output.click(saving_output, [dataset_dropdown], [message_output])
    iblock.title = "å°è‹¹æœwebui"

if __name__ == "__main__":
    iblock.launch()
